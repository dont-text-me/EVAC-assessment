{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5be2416c46d48d69",
   "metadata": {},
   "source": [
    "%Latex \n",
    "\\setcounter{secnumdepth}{0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76badf96318508ee",
   "metadata": {},
   "source": "# Exam number: Y3892609"
  },
  {
   "cell_type": "markdown",
   "id": "15799715004aa624",
   "metadata": {},
   "source": "Install packages"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f5d388ee4d913aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T08:35:28.740140Z",
     "start_time": "2024-05-15T08:35:27.566555Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch pandas matplotlib scikit-learn numpy deap seaborn scipy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167d1a226306bfd",
   "metadata": {
    "collapsed": false
   },
   "source": "# Data examination and pre-processing"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:42.864836Z",
     "start_time": "2024-05-18T19:07:42.650972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "data = pd.read_csv(\"eScooterDemand.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "438d750dc1a242a6",
   "metadata": {},
   "source": [
    "%Latex\n",
    "Some initial observations: \n",
    "\\begin{itemize}\n",
    "\\item The \\verb|Date| column will need processing to a more \"code-friendly\" (i.e. numeric) format, for example, by splitting up the day, month and year field\n",
    "\\item The \\verb|Season| column will be easier to process if the categorical values were converted into numeric ones - a possible technique for achieving this is splitting the column into four \"one-hot\" encoded columns for each season\n",
    "\\item The \\verb|Public Holiday| column, similarly, needs processing to convert the text values into numeric ones - a good representation is 1 for \"Yes\" and 0 for \"No\"\n",
    "\\item The \\verb|HireAvailable| column suggests that some readings were taken when the scheme was not running - it makes little sense to keep these rows for training as no scooters will be rented (by customers) when the scheme is not available\n",
    "\\end{itemize}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3e0452fd44d20",
   "metadata": {
    "collapsed": false
   },
   "source": "## Addressing the data observations"
  },
  {
   "cell_type": "markdown",
   "id": "1cfef60f40438118",
   "metadata": {},
   "source": "### Convert \"Yes/No\" cells to numeric values"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a394ad3670e21d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:45.030424Z",
     "start_time": "2024-05-18T19:07:44.756097Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Public Holiday\"] = data[\"Public Holiday\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "data[\"HireAvailable\"] = data[\"HireAvailable\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "display(Latex(data[[\"Public Holiday\", \"HireAvailable\"]].head().to_latex()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8033a2b6c54d16",
   "metadata": {
    "collapsed": false
   },
   "source": "### One-hot encode season data"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca458cf3a2f258a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:45.828428Z",
     "start_time": "2024-05-18T19:07:45.816211Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for season in [\"Winter\", \"Autumn\", \"Spring\", \"Summer\"]:\n",
    "    data[season] = data[\"Season\"].apply(lambda x: 1 if x == season else 0)\n",
    "del data[\"Season\"]\n",
    "display(Latex(data[[\"Winter\", \"Autumn\", \"Spring\", \"Summer\"]].head().to_latex()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f1b4ed38220524",
   "metadata": {
    "collapsed": false
   },
   "source": "### Break date column into day, month and year columns"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2651809cf17402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:47.677660Z",
     "start_time": "2024-05-18T19:07:47.657625Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"day\"] = data[\"Date\"].apply(lambda x: int(x.split(\"/\")[0]))\n",
    "data[\"month\"] = data[\"Date\"].apply(lambda x: int(x.split(\"/\")[1]))\n",
    "data[\"year\"] = data[\"Date\"].apply(lambda x: int(x.split(\"/\")[2]))\n",
    "del data[\"Date\"]\n",
    "display(Latex(data[[\"day\", \"month\", \"year\"]].head().to_latex()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34223b41ab4b67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scale numeric columns\n",
    "This will keep the range of the data manageable, reducing the need for the algorithm to \"balance\" data of different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b9535aa65d57d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:49.576053Z",
     "start_time": "2024-05-18T19:07:49.137826Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "numeric_columns = [\n",
    "    \"Temp\",\n",
    "    \"Humidity\",\n",
    "    \"Wind speed\",\n",
    "    \"Hour\",\n",
    "    \"Visibility\",\n",
    "    \"Dew point\",\n",
    "    \"Sunshine\",\n",
    "    \"Rain\",\n",
    "    \"Snow\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "]\n",
    "data[numeric_columns] = minmax_scale(data[numeric_columns])\n",
    "display(\n",
    "    Latex(\n",
    "        data[list(set(numeric_columns) - {\"day\", \"month\", \"year\"})]\n",
    "        .head()\n",
    "        .to_latex(float_format=\"%.2f\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a379680a0b1c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Remove rows where the hire scheme is not running\n",
    "This will also allow for deleting the `HireAvailable` column since it will now be guaranteed to be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7da208c6d9753af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:51.465522Z",
     "start_time": "2024-05-18T19:07:51.460765Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_when_available = data.drop(data[data[\"HireAvailable\"] == 0].index)\n",
    "del data_when_available[\"HireAvailable\"]\n",
    "num_dropped_rows = len(data) - len(data_when_available)\n",
    "print(\n",
    "    f\"Removed {num_dropped_rows} rows ({round((num_dropped_rows / len(data)) * 100, 2)}% of all rows)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf7cb856fe84ee",
   "metadata": {},
   "source": "The numbers above suggest that the number of readings taken when the scheme was not operating did not represent a significant proportion of all readings and is therefore unlikely to affect the model's performance on the rest of the data."
  },
  {
   "cell_type": "markdown",
   "id": "c4e4f5c1f0cb8133",
   "metadata": {},
   "source": [
    "### Final observations\n",
    "Now that the data is scaled, the range of values of the `Count` column is significantly greater, requiring large weights in individuals.\n",
    "While not necessarily an issue, converting the data to a lower range may help the individuals keep their representation to a smaller range of values, reducing the search space.\n",
    "A potential way of reducing the range is applying a natural log to the `Count` column. Given that the maximum in this dataset is 3556, the maximum log value will be around 8, significantly reducing the available range. \n",
    "A possible issue may arise with counts of 0 and 1 - the log of 1 is 0 and log of 0 is undefined. A possible solution is to simply not convert counts of zero. While calculating the original value from the converted column could lead to an error of 1, the scale of the original `Count` column makes it possible to argue that an error of one is not significant in the scale of the average number of daily rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8858acf873a15",
   "metadata": {},
   "source": "For additional confidence in the above, count the number of times when `Count` was either 0 or 1"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937e69d492056b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:54.121929Z",
     "start_time": "2024-05-18T19:07:54.118029Z"
    }
   },
   "outputs": [],
   "source": [
    "count_equal_to_zero = len(data_when_available[data_when_available[\"Count\"] == 0])\n",
    "count_equal_to_one = len(data_when_available[data_when_available[\"Count\"] == 1])\n",
    "print(\n",
    "    f\"Count = 0 on {count_equal_to_zero} observations\\nCount = 1 on {count_equal_to_one} observations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cb59aa5d3e7e9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:55.659211Z",
     "start_time": "2024-05-18T19:07:55.647432Z"
    }
   },
   "outputs": [],
   "source": [
    "data_when_available[\"Count\"] = data_when_available[\"Count\"].apply(\n",
    "    lambda x: 0 if x == 0 else np.log(x)\n",
    ")\n",
    "display(Latex(data_when_available[\"Count\"].head().to_latex(float_format=\"%.5f\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d697d4612d43925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:07:58.170844Z",
     "start_time": "2024-05-18T19:07:58.168173Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"New range for the Count column - min: {data_when_available['Count'].min()}, max: {data_when_available['Count'].max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b1fb2db4b55829",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Investigating the degree to which other columns affect the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77517a5c512a6c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:00.047570Z",
     "start_time": "2024-05-18T19:07:59.606270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(35, 16))\n",
    "ax.bar_label(\n",
    "    ax.bar(\n",
    "        list(map(str.capitalize, data_when_available.drop(\"Count\", axis=1).columns)),\n",
    "        data_when_available.corr(\"pearson\")[\"Count\"]\n",
    "        .drop(\"Count\")\n",
    "        .abs()\n",
    "        .sort_values(ascending=False),\n",
    "    ),\n",
    "    fontsize=18,\n",
    ")\n",
    "_ = ax.set_title(\n",
    "    \"Absolute pearson correlation between columns and scooter count\", fontsize=24\n",
    ")\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42594ed8d91baa1",
   "metadata": {},
   "source": [
    "It is clear that the hour and air temperature have the strongest correlation with the number of rentals.\n",
    "This plot suggests possible design choices for individuals in the next section. While the size of the individuals in a neural network representation may not differ significantly with less \"important\" columns (i.e. with more input neurons), a tree-based genetic programming approach can benefit significantly from a reduced number of branches. In that case, a technique like PCA could be used to extract the most informative features from the data while keeping the trees as narrow as possible.\n",
    "\n",
    "Overall, the large number of available variables suggest evolving a neural network as a strong choice for this task, as the large amounts of internal connections will allow the individuals to model relationships between the variables that would be hard to capture otherwise - in a genetic programming approach, the individuals are likely to have extremely large heights, leading to computationally expensive evaluation functions and slowing down the training loop. Furthermore, by comparing the evolved network to one trained using a conventional backpropagation algorithm, the success of the evolutionary algorithm can be easily evaluated and compared to other methods."
   ]
  },
  {
   "cell_type": "raw",
   "id": "68d7f25512b260f4",
   "metadata": {},
   "source": "%Latex \\pagebreak"
  },
  {
   "cell_type": "markdown",
   "id": "f138cb8c6681962d",
   "metadata": {},
   "source": "# Neural network approach - investigating network architectures"
  },
  {
   "cell_type": "markdown",
   "id": "dd74de9a58f26b0b",
   "metadata": {},
   "source": "It may be beneficial to look at the performance of a neural network trained the conventional way to get an idea of the possible performance of one trained using a genetic algorithm"
  },
  {
   "cell_type": "markdown",
   "id": "dd60fb5fbe41b3f0",
   "metadata": {},
   "source": "Split data into a \"training\" and testing set - the training set will be used to evaluate the fitness of the individuals during the evolution process and the testing portion will be used to evaluate the best individual's performance on unseen data"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71675fb006cfa674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:03.090391Z",
     "start_time": "2024-05-18T19:08:03.049712Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data_when_available.drop(\"Count\", axis=1), data_when_available[\"Count\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.8,\n",
    "    random_state=222,  # for repeatable results\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65bce9132d3893",
   "metadata": {},
   "source": "One hidden layer"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3718ef104e58caa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:16.825416Z",
     "start_time": "2024-05-18T19:08:05.663057Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = (\n",
    "    torch.from_numpy(np.float32(X_train)),\n",
    "    torch.from_numpy(np.float32(X_test)),\n",
    "    torch.from_numpy(np.float32(y_train)).unsqueeze(1),\n",
    "    torch.from_numpy(np.float32(y_test)).unsqueeze(1),\n",
    ")\n",
    "\n",
    "model_1_h_l = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X.shape[1], 256, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 1, bias=False),\n",
    "    torch.nn.ReLU(),  # The number of rentals can never go below zero, prevent the model from outputting negative numbers\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(model_1_h_l.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 4000\n",
    "\n",
    "for epoch in range(num_epochs + 1):\n",
    "    # Pass training data through model\n",
    "    y_predict = model_1_h_l(X_train_tensor)\n",
    "    # Compute BCE loss\n",
    "    loss = criterion(y_predict, y_train_tensor)\n",
    "    # Backward pass and gradient step\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if not epoch % 1000:\n",
    "        # Print out the loss every 200 iterations\n",
    "        print(\"epoch {}, loss {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4133876c093d2d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:18.331954Z",
     "start_time": "2024-05-18T19:08:18.327054Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "preds = model_1_h_l(X_test_tensor)\n",
    "\n",
    "print(\n",
    "    rf\"The R^2 score of the 1 hidden layer model on the unseen dataset is {r2_score(preds.tolist(), y_test_tensor.tolist())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab451b777c47b9",
   "metadata": {},
   "source": "Two hidden layers"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cbdafde58cd8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:28.566555Z",
     "start_time": "2024-05-18T19:08:20.026993Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2_h_l = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_train.shape[1], 128, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 32, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 1, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(model_2_h_l.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 4000\n",
    "\n",
    "for epoch in range(num_epochs + 1):\n",
    "    # Pass training data through model\n",
    "    y_predict = model_2_h_l(X_train_tensor)\n",
    "    # Compute BCE loss\n",
    "    loss = criterion(y_predict, y_train_tensor)\n",
    "    # Backward pass and gradient step\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if not epoch % 1000:\n",
    "        # Print out the loss every 200 iterations\n",
    "        print(\"epoch {}, loss {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a5d69cf25fd7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:30.225878Z",
     "start_time": "2024-05-18T19:08:30.220824Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model_2_h_l(X_test_tensor)\n",
    "\n",
    "print(\n",
    "    f\"The R^2 score of the 2 hidden layer model on the unseen dataset is {r2_score(preds.tolist(), y_test_tensor.tolist())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3aef82fc7d160",
   "metadata": {},
   "source": [
    "From these experiments, it is evident that even a single hidden layer is capable of representing the data well. The value of the $r^2$ score suggests that the model is able to explain a significant portion of the variance in the dataset.\n",
    "This suggests that evolving a neural network is a good choice for this problem, with a single hidden layer resulting in relatively small individuals.\n",
    "Furthermore, the processing done above reduces the search space as the model is able to make predictions with smaller weights."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3b759d08bd1c968",
   "metadata": {},
   "source": "%Latex \\pagebreak"
  },
  {
   "cell_type": "markdown",
   "id": "7667552c153ac433",
   "metadata": {
    "collapsed": false
   },
   "source": "# Evolutionary algorithm"
  },
  {
   "cell_type": "markdown",
   "id": "e7685a1008ce88cf",
   "metadata": {},
   "source": "## Individuals and fitnesses\n"
  },
  {
   "cell_type": "markdown",
   "id": "7563c5b668812daf",
   "metadata": {},
   "source": [
    "To avoid keeping an entire population of networks in memory, there will only exist a single neural network. \n",
    "The individuals will be represented by lists of floating-point numbers, with the length of the list matching the number of parameters in the network.\n",
    "\n",
    "To evaluate an individual, its \"genome\" will be reshaped to fit the structure of the network. The individual's fitness will be the model's mean squared error on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d41ad7d0ee423796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:32.642990Z",
     "start_time": "2024-05-18T19:08:32.638483Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 256  # The number of hidden neurons used in the experiment above\n",
    "MODEL = torch.nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"fc1\", torch.nn.Linear(X.shape[1], HIDDEN_LAYER_SIZE, bias=False)),\n",
    "            (\"relu1\", torch.nn.ReLU()),\n",
    "            (\"fc2\", torch.nn.Linear(HIDDEN_LAYER_SIZE, 1, bias=False)),\n",
    "            (\"relu2\", torch.nn.ReLU()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "IND_SIZE = len(torch.flatten(MODEL.fc1.weight)) + len(torch.flatten(MODEL.fc2.weight))\n",
    "layer_1_size, layer_2_size = (\n",
    "    len(torch.flatten(MODEL.fc1.weight)),\n",
    "    len(torch.flatten(MODEL.fc2.weight)),\n",
    ")\n",
    "print(\n",
    "    f\"Each individual consists of {IND_SIZE} numbers representing the model weights, of which {layer_1_size} represent the 1st layer and {layer_2_size} the second\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751dd93fa5d38e8a",
   "metadata": {},
   "source": "Some simple tests of setting the model with an individual's weights"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af78492cbc45c0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:34.166919Z",
     "start_time": "2024-05-18T19:08:34.161560Z"
    }
   },
   "outputs": [],
   "source": [
    "test_zeros = list(\n",
    "    np.zeros(IND_SIZE)\n",
    ")  # Based on experience from the practicals, it's better to keep individuals as lists as opposed to numpy arrays\n",
    "test_ones = list(np.ones(IND_SIZE))\n",
    "test_ones_and_zeros = list(\n",
    "    np.concatenate((np.zeros(layer_1_size), np.ones(layer_2_size)))\n",
    ")  # This should set all layer 1 weights to zeros and layer 2 weights to ones\n",
    "\n",
    "\n",
    "def set_weights(individual):\n",
    "    \"\"\"\n",
    "    Set the weights of the singleton model to the genes of the given individual\n",
    "    :param individual: a list of size IND_SIZE\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        MODEL.fc1.weight.data = torch.from_numpy(\n",
    "            np.float32(individual[0:layer_1_size]).reshape(\n",
    "                (HIDDEN_LAYER_SIZE, X.shape[1])\n",
    "            )\n",
    "        )\n",
    "        MODEL.fc2.weight.data = torch.from_numpy(\n",
    "            np.float32(individual[layer_1_size : layer_1_size + layer_2_size]).reshape(\n",
    "                (1, HIDDEN_LAYER_SIZE)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "set_weights(test_ones_and_zeros)\n",
    "assert (\n",
    "    torch.sum(MODEL.fc1.weight) == 0 and torch.sum(MODEL.fc2.weight) == layer_2_size\n",
    ")  # Test the reshaping works correctly: first layer is all zeros and second layer is all ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579deffbbd3104",
   "metadata": {},
   "source": "Evaluating the fitness of an individual - using the mean squared error as the metric"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1120e845ca1fbaf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:35.640714Z",
     "start_time": "2024-05-18T19:08:35.633586Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def evaluate(individual):\n",
    "    with torch.no_grad():\n",
    "        set_weights(individual)\n",
    "        preds = MODEL(X_train_tensor)\n",
    "    return (\n",
    "        mse(preds, y_train_tensor).item(),\n",
    "    )  # DEAP requires the use of tuples in the fitness\n",
    "\n",
    "\n",
    "print(evaluate(test_ones), evaluate(test_zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d9406a1301fb9",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "The code below will populate a DEAP toolbox with the necessary functions to keep track and describe the success of a population of individuals.\n",
    "\n",
    "To create a new individual, a list of size IND_SIZE will be populated with numbers sampled from a gaussian distribution of floating-point numbers. Keeping the mean of the distribution to a lower range allows the networks to be initialised with smaller weights and is a strategy commonly used in more conventional machine learning scenarios.\n",
    "\n",
    "Given that the fitness of an individual is given by its mean squared error, the objective of the evolutionary algorithm is the minimisation of the individuals' fitnesses. For this, the weight of the fitness will be set to -1.\n",
    "\n",
    "It should be noted that evolving a neural network is unlikely to benefit from crossover: changing the weights in such a drastic way can significantly affect the performance of the new individuals and lead to an unstable training curve. Instead, each individual in the population will mutate, allowing for more gradual changes in their genome.\n",
    "It is also for this reason that a simple 1-dimensional list is sufficient to represent the individuals - a nested structure (e.g. a sublist per layer) will provide little benefit and will make the representation more complex.\n",
    "\n",
    "Given the size of the individual and the network's sensitivity to changes in weights, the rate of mutation is likely to stay relatively low.\n",
    "\n",
    "The best individuals will be entered in a hall of fame (a feature of DEAP), which will be used to retrieve the best ever recorded individual after the training is complete. This allows the algorithm to be more resilient to mutation negatively affecting the individuals' performance as the best genome will not be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3156bb8d4679105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:12:14.802963Z",
     "start_time": "2024-05-18T19:12:14.800228Z"
    }
   },
   "outputs": [],
   "source": [
    "from deap import base, creator, tools\n",
    "\n",
    "import random\n",
    "from warnings import simplefilter, resetwarnings\n",
    "\n",
    "simplefilter(\n",
    "    \"ignore\", RuntimeWarning\n",
    ")  # Hide warnings about overwriting classes in order to avoid showing the local username on the PDF\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "resetwarnings()  # Stop hiding runtime warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6db9e56565100164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T19:08:49.137726Z",
     "start_time": "2024-05-18T19:08:49.134579Z"
    }
   },
   "outputs": [],
   "source": [
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_float\", random.gauss, -1.0, 1.0)\n",
    "toolbox.register(\n",
    "    \"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=IND_SIZE\n",
    ")\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=25)\n",
    "\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0.0, sigma=0.5, indpb=0.003)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "hof = tools.HallOfFame(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d9c99ad152b2cd8",
   "metadata": {},
   "source": "%Latex \\pagebreak"
  },
  {
   "cell_type": "markdown",
   "id": "8b3cf74c4627ffe9",
   "metadata": {},
   "source": [
    "## Investigating the effect of parameters on algorithm performance\n",
    "\n",
    "Since crossover is not part of the algorithm, the main parameters influencing the performance of the model will be the population size, mutation rate and the number of participants in each tournament when selecting the offspring. To test their effects on the performance, it can be useful to setup a \"grid search\", where each experiment is ran several times to account for the stochastic nature of evolutionary algorithms. From there, the statistics of the gathered runs can be examined to determine the combination of parameters that is the most likely to lead to the best results when left to run for a large number of generations. To keep computational costs and run time of the search low, the number of generations in the experiment will be limited to 150.\n",
    "\n",
    "Given that each individual is guaranteed to mutate as a whole, it is reasonable to keep the individual genes' mutation probabilities small - the relatively large size of the individuals combined with a high mutation rate will make evolution unstable. Therefore, mutation rates will not exceed 1% for an individual gene. (affecting around 46 values in each individual representing a 256-neuron wide network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6b3a7f2a2acdccc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:43:33.020706Z",
     "start_time": "2024-05-15T11:30:10.173166Z"
    }
   },
   "outputs": [],
   "source": [
    "experiments = {}\n",
    "NGEN_SEARCH = 150\n",
    "for pop_size in [150, 200, 250]:\n",
    "    for mutation_rate in [0.001, 0.002, 0.003, 0.01]:\n",
    "        for tournament_size in [10, 15, 25]:\n",
    "            experiment = f\"Population size: {pop_size}, mutation rate: {mutation_rate}, tournament size: {tournament_size}\"\n",
    "            print(f\"Conducting experiment {experiment}\")\n",
    "            for i in range(3):\n",
    "                hof.clear()\n",
    "                if experiment not in experiments.keys():\n",
    "                    experiments[experiment] = []\n",
    "                # Create population and evaluate initial fitnesses\n",
    "                pop = toolbox.population(n=pop_size)\n",
    "                hof = tools.HallOfFame(1)\n",
    "                fitnesses = [toolbox.evaluate(indiv) for indiv in pop]\n",
    "                for ind, fit in zip(pop, fitnesses):\n",
    "                    ind.fitness.values = fit\n",
    "\n",
    "                toolbox.register(\n",
    "                    \"mutate\", tools.mutGaussian, mu=0.0, sigma=0.5, indpb=mutation_rate\n",
    "                )\n",
    "                toolbox.register(\n",
    "                    \"select\", tools.selTournament, tournsize=tournament_size\n",
    "                )\n",
    "\n",
    "                for g in range(NGEN_SEARCH):\n",
    "                    offspring = toolbox.select(pop, len(pop))\n",
    "                    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "                    for mutant in offspring:\n",
    "                        toolbox.mutate(mutant)\n",
    "                        del mutant.fitness.values\n",
    "\n",
    "                    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "                    fitnesses = [toolbox.evaluate(indiv) for indiv in invalid_ind]\n",
    "                    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                        ind.fitness.values = fit\n",
    "\n",
    "                    pop[:] = offspring\n",
    "                    hof.update(pop)\n",
    "\n",
    "                best_ind = hof.items[0]\n",
    "                set_weights(best_ind)\n",
    "                with torch.no_grad():\n",
    "                    preds = MODEL(X_test_tensor)\n",
    "                experiments[experiment].append(\n",
    "                    {\n",
    "                        \"lowest fitness: \": best_ind.fitness.values[0],\n",
    "                        \"highest r^2 on test data\": r2_score(\n",
    "                            y_test_tensor.tolist(), preds.tolist()\n",
    "                        ),\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e3826b1251640",
   "metadata": {},
   "source": "Show the most successful experiments: sort the produced dict by the median highest $r^2$ score over the three runs"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f968789dbb7b3693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T15:33:19.697409Z",
     "start_time": "2024-05-15T15:33:19.692756Z"
    }
   },
   "outputs": [],
   "source": [
    "list(\n",
    "    set(\n",
    "        dict(\n",
    "            sorted(\n",
    "                experiments.items(),\n",
    "                key=lambda item: np.median(\n",
    "                    [it[\"highest r^2 on test data\"] for it in item[1]]\n",
    "                ),\n",
    "                reverse=True,\n",
    "            )\n",
    "        ).keys()\n",
    "    )\n",
    ")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3c823476e2363",
   "metadata": {},
   "source": "It can be seen that a population of 200 is seen in three of the top 5 scoring experiments, making it a good choice for a potential longer run. Interestingly, the top result has been achieved with a relatively high mutation rate. While this could suggest that such high rates can be beneficial for a longer run, sticking with a lower rate (e.g. 0.003) can make the training process smoother and reduce the chances of a mutation making a good individual worse later in the training loop. Finally, a tournament size of 15 is present in 2/5 entries in the top 5, making it a good starting point for a longer run."
  },
  {
   "cell_type": "markdown",
   "id": "6276db9ec601d650",
   "metadata": {},
   "source": [
    "### Initialisation strategies\n",
    "\n",
    "As mentioned above, initialisation is an important part of training a neural network. Therefore, it could be assumed that sampling from different distributions to create the initial population can affect the results.\n",
    "While the search space for this parameter is quite large, it could be useful to look at 3 gaussian distributions with means of -1, 0 and 1 and a standard deviation of 1. This will provide a rough overview of the best sampling strategies. To change the distribution, the \"attr_float\" tool of the toolbox will be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d95b464bc69941a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:33:39.161230Z",
     "start_time": "2024-05-18T18:21:46.860823Z"
    }
   },
   "outputs": [],
   "source": [
    "initialisation_experiments = {}\n",
    "NGEN_SEARCH = 150\n",
    "for mu in [-1.0, 0.0, 1.0]:\n",
    "    toolbox.register(\"attr_float\", random.gauss, mu=mu, sigma=1.0)\n",
    "    toolbox.register(\n",
    "        \"individual\",\n",
    "        tools.initRepeat,\n",
    "        creator.Individual,\n",
    "        toolbox.attr_float,\n",
    "        n=IND_SIZE,\n",
    "    )\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    experiment = f\"mu = {mu}\"\n",
    "    print(\"Running experiment \", experiment)\n",
    "    for i in range(3):\n",
    "        hof.clear()\n",
    "        if experiment not in initialisation_experiments.keys():\n",
    "            initialisation_experiments[experiment] = []\n",
    "        # Create population and evaluate initial fitnesses\n",
    "        pop = toolbox.population(n=200)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        fitnesses = [toolbox.evaluate(indiv) for indiv in pop]\n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "\n",
    "        for g in range(NGEN_SEARCH):\n",
    "            offspring = toolbox.select(pop, len(pop))\n",
    "            offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "            for mutant in offspring:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "            fitnesses = [toolbox.evaluate(indiv) for indiv in invalid_ind]\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit\n",
    "\n",
    "            pop[:] = offspring\n",
    "            hof.update(pop)\n",
    "\n",
    "        best_ind = hof.items[0]\n",
    "        set_weights(best_ind)\n",
    "        with torch.no_grad():\n",
    "            preds = MODEL(X_test_tensor)\n",
    "        initialisation_experiments[experiment].append(\n",
    "            {\n",
    "                \"lowest fitness: \": best_ind.fitness.values[0],\n",
    "                \"highest r^2 on test data\": r2_score(\n",
    "                    y_test_tensor.tolist(), preds.tolist()\n",
    "                ),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43bb13544794078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T18:59:50.971807Z",
     "start_time": "2024-05-18T18:59:50.969196Z"
    }
   },
   "outputs": [],
   "source": [
    "average_r2_scores = {}\n",
    "for experiment, results in initialisation_experiments.items():\n",
    "    print(\n",
    "        f'The average best r^2 score for an initial gaussian distribution with {experiment} is {np.mean([result[\"highest r^2 on test data\"] for result in results])}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32fc88af63be9a",
   "metadata": {},
   "source": "From this, it is clear that an initial distribution with a mean of -1 allows the models to achieve better results on the test dataset. While the result for a mean of zero is not extreme and could suggest that improvement is possible after more generations, the results of an initial distribution with a mean of -1 suggest that the algorithm was able to begin producing viable individuals much quicker."
  },
  {
   "cell_type": "markdown",
   "id": "dc2cd302146c3acf",
   "metadata": {},
   "source": [
    "## Running the algorithm (with optimal parameters)\n",
    "\n",
    "The following code block will put together the insights gathered during the experiments above and run the resulting algorithm for a longer number of generations. Then, the best individual will be tested and compared to a neural network trained via backpropagation to evaluate the model against a more commonly used algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7487538a6e762480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:11.773694Z",
     "start_time": "2024-05-15T16:13:58.363898Z"
    }
   },
   "outputs": [],
   "source": [
    "NGEN = 1000\n",
    "toolbox.register(\"attr_float\", random.gauss, mu=-1.0, sigma=1.0)\n",
    "toolbox.register(\n",
    "    \"individual\", tools.initRepeat, creator.Individual, toolbox.attr_float, n=IND_SIZE\n",
    ")\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=15)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0.0, sigma=0.5, indpb=0.003)\n",
    "stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "display_logbook = (\n",
    "    tools.Logbook()\n",
    ")  # Logbook for displaying the scores as training happens - at a reduced rate\n",
    "plotting_logbook = (\n",
    "    tools.Logbook()\n",
    ")  # Logbook for visualising curves - records every single generation\n",
    "pop = toolbox.population(n=200)\n",
    "hof.clear()\n",
    "# Evaluate the fitness of the population prior to running the main algorithm\n",
    "fitnesses = [toolbox.evaluate(indiv) for indiv in pop]\n",
    "for ind, fit in zip(pop, fitnesses):\n",
    "    ind.fitness.values = fit\n",
    "\n",
    "\n",
    "display_logbook.header = \"gen\", \"avg\", \"std\", \"min\", \"max\"\n",
    "plotting_logbook.header = \"gen\", \"avg\", \"std\", \"min\", \"max\"\n",
    "for g in range(NGEN):\n",
    "    offspring = toolbox.select(pop, len(pop))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for mutant in offspring:\n",
    "        toolbox.mutate(mutant)\n",
    "        del mutant.fitness.values\n",
    "\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = [toolbox.evaluate(indiv) for indiv in invalid_ind]\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    pop[:] = offspring\n",
    "    record = stats.compile(pop)\n",
    "    hof.update(pop)\n",
    "    plotting_logbook.record(gen=g, **record)\n",
    "    if not g % 50:  # Display logbook every 50 epochs to avoid bloating the PDF\n",
    "        display_logbook.record(gen=g, **record)\n",
    "        print(display_logbook.stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "39509fe1ac3ad71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:16.500100Z",
     "start_time": "2024-05-15T16:22:16.408921Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(plotting_logbook.select(\"min\"))\n",
    "ax.set_xlabel(\"Epoch\", fontsize=20)\n",
    "ax.set_ylabel(\"Fitness\", fontsize=20)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "_ = ax.set_title(\"Best individual (lowest fitness) per epoch\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1cd7735133492",
   "metadata": {},
   "source": [
    "It is clear that the performance of the indivuduals is consistently improving: the plot above shows the minimum fitness (i.e. the mean squared error on the training set) across the epochs.\n",
    "The best individuals have been tracked by the `hof` variable, which allows accessing the best individual to have ever been recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543fcade5ee06b",
   "metadata": {},
   "source": "To evaluate the best individual on unseen data, the below cell will calculate the $r^2$ score of the individual on the portion of the original data held back for testing"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7d59e06f1469536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:28.154613Z",
     "start_time": "2024-05-15T16:22:28.149257Z"
    }
   },
   "outputs": [],
   "source": [
    "best_ind = hof.items[0]\n",
    "set_weights(best_ind)\n",
    "print(f\"The best individual' fitness is {best_ind.fitness.values[0]}\")\n",
    "with torch.no_grad():\n",
    "    preds = MODEL(X_test_tensor)\n",
    "    print(\n",
    "        f\"The individual's r^2 score on an unseen portion of the dataset is {r2_score(y_test_tensor.tolist(), preds.tolist())}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6518e07becc2c",
   "metadata": {},
   "source": [
    "This score suggests that the evolved network is able to explain about 74% of the variance of the count of scooters.\n",
    "While not a perfect fit to the data, this result is close to the score obtained via \"conventional\" training, suggesting that the evolutionary algorithm is capable of achieving similar quality results.\n",
    "Another useful metric can be the mean absolute error on the testing dataset. This will allow us to see the performance of the network in the context of its target i.e. to see the mean number of scooters it is wrong by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "afaa7645c875ede9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:30.979071Z",
     "start_time": "2024-05-15T16:22:30.974820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "preds_exponent = np.round(\n",
    "    np.exp(preds)\n",
    ")  # Having fractions of a scooter rented on a given day is impossible, convert to the nearest integer\n",
    "actual_exponent = np.round(np.exp(y_test))\n",
    "best_ind_mae = mean_absolute_error(actual_exponent, preds_exponent)\n",
    "print(best_ind_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d8c54b23e594f",
   "metadata": {},
   "source": "On average, the trained network is mistaken by ~252 scooters. To see how this may affect the model in the \"real world\", it may be useful to plot the distribution of scooter counts."
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6f20c57a879a10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:35.071765Z",
     "start_time": "2024-05-15T16:22:35.068694Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_nn = model_1_h_l(X_test_tensor)\n",
    "mae_nn = torch.nn.L1Loss()(torch.exp(preds_nn), torch.exp(y_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9c8390a57b4a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:22:36.949948Z",
     "start_time": "2024-05-15T16:22:36.643693Z"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(16, 9))\n",
    "ax.hist(np.exp(data_when_available[\"Count\"]), bins=100)\n",
    "ax.set_xlabel(\"Scooter count\", fontsize=20)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=20)\n",
    "ax.plot(\n",
    "    [best_ind_mae] * 400,\n",
    "    range(400),\n",
    "    label=\"Mean absolute error value (evolutionary algorithm)\",\n",
    ")\n",
    "ax.plot(\n",
    "    [mae_nn.item()] * 400,\n",
    "    range(400),\n",
    "    label=\"Mean absolute error value (conventional training)\",\n",
    "    color=\"purple\",\n",
    ")\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "_ = ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa900d4db5272fc6",
   "metadata": {},
   "source": [
    "The above plot shows that the model's mean error is greater than a large number of observations in the dataset. Despite that, the mean absolute error is close to that of the conventionally trained network.\n",
    "\n",
    "While the results obtained in these experiments are not perfect, it is evident that the evolutionary approach is a powerful tool for evolving neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46467288086b92b",
   "metadata": {},
   "source": "It can also be informative to look at the best individual's distribution of errors on the testing dataset as well as the median error"
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c009c519b7aae4e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:23:46.003309Z",
     "start_time": "2024-05-15T16:23:45.794542Z"
    }
   },
   "outputs": [],
   "source": [
    "set_weights(best_ind)\n",
    "with torch.no_grad():\n",
    "    preds = MODEL(X_test_tensor)\n",
    "preds_exponent = torch.round(torch.exp(preds))\n",
    "actual_exponent = torch.round(torch.exp(y_test_tensor))\n",
    "diffs = torch.abs(preds_exponent - actual_exponent)\n",
    "diffs = diffs.squeeze().tolist()\n",
    "median_diff = np.median(diffs)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16, 12))\n",
    "ax.hist(diffs, bins=100)\n",
    "ax.plot(\n",
    "    [median_diff] * 180,\n",
    "    range(180),\n",
    "    label=f\"Median error ({median_diff})\",\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.set_xlabel(\"Absolute error (scooters)\", fontsize=20)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=20)\n",
    "ax.set_title(\"Distribution of absolute errors on the testing set\", fontsize=20)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "_ = ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349e3d5f4e0960f",
   "metadata": {},
   "source": "It is evident that the testing dataset contains several outliers which are driving up the mean error. This explains the difference between the mean and the median errors."
  },
  {
   "cell_type": "markdown",
   "id": "a8252f11ff15654",
   "metadata": {},
   "source": "Finally, it can be useful to examine the performance of the conventionally trained 1-hidden-layer model compared to its evolved counterpart"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e19a255fb46cc541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:23:50.569327Z",
     "start_time": "2024-05-15T16:23:50.566081Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_nn = model_1_h_l(X_test_tensor)\n",
    "preds_nn_exp = torch.exp(preds_nn)\n",
    "diffs_nn = torch.abs(preds_nn_exp - torch.exp(y_test_tensor))\n",
    "diffs_nn = diffs_nn.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "14bdbbc0f375ac1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:23:52.258041Z",
     "start_time": "2024-05-15T16:23:52.154153Z"
    }
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(16, 9))\n",
    "bp = ax.boxplot((diffs, diffs_nn), vert=False, widths=0.7)\n",
    "for cap in bp[\"caps\"]:\n",
    "    cap.set(color=\"orange\", linewidth=2, zorder=3)\n",
    "ax.set_yticklabels([\"Evolutionary\\nalgorithm\", \"Conventional\\ntraining\"], fontsize=20)\n",
    "ax.set_xlabel(\"Absolute error\", fontsize=20)\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "_ = ax.set_title(\"Distribution of errors for different training methods\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f77be9b4036dc",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "8c256e79863fa85a",
   "metadata": {},
   "source": [
    "The above plot, again, suggests that the results gathered using the evolutionary algorithm are of similar quality as the ones gathered using conventional training. \n",
    "The median errors across both methods are similar, though the 75th percentile error and interquartile range on the evolved network are greater. Both models show a relatively large number of outliers, suggesting that the fitting potential of the architecture itself is not enough to capture the full range of data, suggesting that a larger network could me more successful at fitting to the dataset.\n",
    "\n",
    "Despite that, the small difference in median error suggest that evolving a neural network provided a relatively well-fitted model on par with a model trained via the usual backpropagation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf321792a91f22b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Overall, it is clear that the evolutionary algorithm was able to produce a consistently improving population which, when fitted to a neural network, is able to closely match the results gained via common backpropagation algorithms. It should also be noted that the evolutionary algorithm took fewer epochs to achieve its result - requiring 900 generations compared to the 4000 epochs needed for backpropagation. While not a direct comparison due to each algorithm's different computational requirements, it is clear that evolutionary algorithms can quickly find a solution close to optimal given a suitably narrow search space provided by a compact representation.\n",
    "\n",
    "The size of the network was kept relatively small, with the mean absolute error remaining an area of improvement should the model be used for any critical tasks. Despite that, it is commonly believed that larger (that is, wider, deeper or a combination of both) networks are able to fit the data better.\n",
    "Care must be taken when training a larger network, especially that with a deeper architecture, as the size of each individual will grow at an exponential rate, leading to an increased memory and computational cost.\n",
    "\n",
    "A future improvement to the algorithm could be seen in evolving the architecture of the network itself along with the weights, in an algorithm such as [NEAT](https://nn.cs.utexas.edu/?stanley:ec02). This, however, goes beyond the capabilities offered by DEAP.\n",
    "The results presented here can therefore be seen as a \"proof of concept\" and a display of the capabilities of evolutionary algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
